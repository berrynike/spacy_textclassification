{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from os.path import join, exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting WordEmbeddings with Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/Export_KI_ALL_Tickets_202005121335.csv\"\n",
    "assert exists(data_path)\n",
    "save_path = \"data/\"\n",
    "save_name = \"vectors_de_en_core_md_np_stopwords.csv\"\n",
    "\n",
    "# set to true in case you want to do preprocessing before extracting the text-vectors\n",
    "remove_stop_words = False\n",
    "remove_punctuation = False\n",
    "lemmatize = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "['tagger', 'parser', 'ner']\n",
      "de\n",
      "['tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# english and german pretrained  core models with bulit in word-vectors\n",
    "# run ($ python -m spacy download en_core_web_md) to download model first\n",
    "nlp_en_md = spacy.load(\"en_core_web_md\")\n",
    "print(nlp_en_md.lang)\n",
    "print(nlp_en_md.pipe_names)\n",
    "nlp_de_md = spacy.load(\"de_core_news_md\")\n",
    "print(nlp_de_md.lang)\n",
    "print(nlp_de_md.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add optional Preprocessing Pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "older @ hello 55 searching and ):VPN three plus Internet user"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for testing wether pipe works as intended\n",
    "test_text = \"older @ hello 55 searching and ):VPN three plus Internet user\"\n",
    "doc = nlp_en_md(test_text)\n",
    "doc_vec_before = doc.vector\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if remove_stop_words:\n",
    "    \n",
    "    def stopword_remover(doc):\n",
    "        new_doc = spacy.tokens.Doc(doc.vocab, words=[token.text for token in doc if not token.is_stop])\n",
    "        return new_doc\n",
    "    \n",
    "    def add_stopword_remover_to_pipe(model):\n",
    "        if \"stopword_remover\" not in model.pipe_names:\n",
    "            model.add_pipe(stopword_remover, first=True, name=\"stopword_remover\")\n",
    "            print(model.pipe_names)\n",
    "    \n",
    "    add_stopword_remover_to_pipe(nlp_de_md)\n",
    "    add_stopword_remover_to_pipe(nlp_en_md)\n",
    "\n",
    "if remove_punctuation:\n",
    "    \n",
    "    def punctuation_remover(doc):\n",
    "        new_doc = spacy.tokens.Doc(doc.vocab, words=[token.text for token in doc if not token.is_punct])\n",
    "        return new_doc\n",
    "    \n",
    "    def add_punctuation_remover_to_pipe(model):\n",
    "        if \"punctuation_remover\" not in model.pipe_names:\n",
    "            model.add_pipe(punctuation_remover, first=True, name=\"punctuation_remover\")\n",
    "            print(model.pipe_names)\n",
    "    \n",
    "    add_punctuation_remover_to_pipe(nlp_de_md)\n",
    "    add_punctuation_remover_to_pipe(nlp_en_md)\n",
    "\n",
    "\n",
    "if lemmatize:\n",
    "    \n",
    "    def lemmatizer(doc):\n",
    "        new_doc = spacy.tokens.Doc(doc.vocab, words=[token.lemma_ for token in doc])\n",
    "        return new_doc\n",
    "    \n",
    "    def add_lemmatizer_to_pipe(model):\n",
    "        if \"lemmatizer\" not in model.pipe_names:\n",
    "            model.add_pipe(lemmatizer, first=True, name=\"lemmatizer\")\n",
    "            print(model.pipe_names)\n",
    "    \n",
    "    add_lemmatizer_to_pipe(nlp_de_md)\n",
    "    add_lemmatizer_to_pipe(nlp_en_md)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "older @ hello 55 searching and ):VPN three plus Internet user\n",
      "older @ hello 55 searching and ):VPN three plus Internet user\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# for testing wether pipe works as intended\n",
    "doc = nlp_en_md(test_text)\n",
    "doc_vec_after = doc.vector\n",
    "print(test_text)\n",
    "print(doc)\n",
    "print((doc_vec_before == doc_vec_after).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only if you need to remove any pipeobject for some reason\n",
    "# nlp_en_md.remove_pipe('lemmatizer')\n",
    "# nlp_en_md.remove_pipe('punctuation_remover')\n",
    "# nlp_en_md.remove_pipe('stopword_remover')\n",
    "# nlp_de_md.remove_pipe('lemmatizer')\n",
    "# nlp_de_md.remove_pipe('punctuation_remover')\n",
    "# nlp_de_md.remove_pipe('stopword_remover')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cols = [\"Zusammenfassung\", \"Description\"]\n",
    "df = pd.read_csv(data_path, usecols = use_cols)\n",
    "# fill nan with empty string\n",
    "df = df.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:1000] # clip data for test purposes\n",
    "texts_short = df[\"Zusammenfassung\"].values\n",
    "texts_long = df[\"Description\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the Text with Spacy Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_docs(nlp_model, texts):\n",
    "    return list(nlp_model.pipe(texts,disable=[\"tagger\", \"parser\", \"ner\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for processing Zusammenfassung EN:  14 seconds\n",
      "Time for processing Zusammenfassung DE:  12 seconds\n",
      "Time for processing Description EN:  10 seconds\n",
      "Time for processing Description DE:  1 seconds\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "docs_short_en = texts_to_docs(nlp_en_md, texts_short)\n",
    "end = datetime.now()\n",
    "print(\"Time for processing Zusammenfassung EN: \",(end-start).seconds, \"seconds\")\n",
    "start = datetime.now()\n",
    "docs_short_de = texts_to_docs(nlp_de_md, texts_short)\n",
    "end = datetime.now()\n",
    "print(\"Time for processing Zusammenfassung DE: \",(end-start).seconds, \"seconds\")\n",
    "\n",
    "start = datetime.now()\n",
    "docs_long_en = texts_to_docs(nlp_en_md, texts_long)\n",
    "end = datetime.now()\n",
    "print(\"Time for processing Description EN: \",(end-start).seconds, \"seconds\")\n",
    "start = datetime.now()\n",
    "docs_long_de = texts_to_docs(nlp_de_md, texts_long)\n",
    "end = datetime.now()\n",
    "print(\"Time for processing Description DE: \",(end-start).seconds, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vectors(docs):\n",
    "    return [doc.vector for doc in docs]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectors_short_en = extract_vectors(docs_short_en)\n",
    "doc_vectors_short_de = extract_vectors(docs_short_de)\n",
    "doc_vectors_long_en = extract_vectors(docs_short_en)\n",
    "doc_vectors_long_de = extract_vectors(docs_short_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append Vectors to Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Zusammenfassung_Vector_DE\"] = doc_vectors_short_de\n",
    "df[\"Zusammenfassung_Vector_EN\"] = doc_vectors_short_en\n",
    "df[\"Description_Vector_DE\"] = doc_vectors_long_de\n",
    "df[\"Description_Vector_EN\"] = doc_vectors_long_en\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(join(save_path,save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
